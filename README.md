# Samsung EnnovateX 2025 AI Challenge Submission
## Project: SnapSense - Instant Image-to-Speech Description for Accessibility
SnapSense is a lightweight, responsive desktop application designed to bridge the digital divide for visually impaired users. It provides instant, context-aware descriptions of any image on the screen, whether it's a screenshot or a local file. With a single click, users can capture a portion of their screen, get an AI-generated caption, and have it read aloud, making graphical content accessible to everyone.

- **Problem Statement** - *(Crafting the Next Generation of Human-AI Interaction on the Edge
Let's go beyond model and traditional interaction capabilities on any edge device. A solution that addresses a real-world problem by leveraging on-device Generative AI, while pioneering novel, effective, and intuitive Human-AI InteractionÂ (H-AI).)*
- **Team name** - *(Cognita)*
- **Team members (Names)** - *Parth Mani Sharma*, *Anjali Singh*
- **Demo Video Link** - *(Upload the Demo video on Youtube as a public or unlisted video and share the link. Google Drive uploads or any other uploads are not allowed.)*


### Project Artefacts

- **Technical Documentation** -  Docs *(All technical details are in markdown files inside the docs folder.)*
- **Source Code** - Source *(All source code is located in the src folder. The code is executable and tested on Linux.)*
- **Models Used** - (vikhyatk/moondream2)[https://huggingface.co/vikhyatk/moondream2] *(A lightweight and fast vision model, perfect for running on edge devices.)*
- **Models Published** -  N/A *(This project leverages an existing open-weight model.)*
- **Datasets Used** - *(The moondream2 model was trained on the Symbolic-Mind/Symbolic-9M dataset.)*
- **Datasets Published** - N/A *(No new datasets were created for this project.)*
